<!DOCTYPE html>
<html lang="en" ng-app="app">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Fact Checking :: Crawler :: Crawler</title>
  <script src="/pages/js/jquery-3.3.1.min.js"></script>
  <script src="/pages/js/jquery-ui.min.js"></script>
  <link href="/pages/css/jquery-ui.min.css" rel="stylesheet">
  <link href="/pages/css/jquery-ui.structure.min.css" rel="stylesheet">
  <link href="/pages/css/jquery-ui.theme.min.css" rel="stylesheet">
  <link href="/pages/css/app.css" rel="stylesheet">
</head>

<body bgcolor="#d3d3d3">
<table width="100%" cellpadding="8px">
  <tr>
    <td class="nopadding" colspan="2">
      <div class="topnav">
        <a href="/pages/index.html">Home</a> |
        <a href="/pages/graph.html">Crawled Graph</a> |
        <a href="/pages/templates/fact-check.html">Fact Check</a> |
        <a class="active" href="/pages/templates/crawler.html">Crawler</a> |
        <a href="/pages/templates/squirrel.html">Squirrel</a> |
        <a href="/pages/templates/about.html">About Us</a>
      </div>
    </td>
  </tr>
</table>

<div class="container">
  <div class="markdown-body">
    <h1>
      <a id="user-content-crawler" class="anchor" href="#crawler" aria-hidden="true"></a>Crawler Subgroup
    </h1>

    <p> Mentor: <a href="michael.roeder@uni-paderborn.de">Michael Röder</a>
    </p>

    <h3>
      <a id="user-content-vision" class="anchor" href="#Vision" aria-hidden="true"></a>Vision
    </h3>

    <ul>
      <li>Manageable 24/7 crawling</li>
      <li>Data is written into a large triple store</li>
    </ul>

    <h3>
      <a id="user-content-requirements" class="anchor" href="#requirements" aria-hidden="true"></a>Requirements
    </h3>

    <ul>
      <li>24/7 crawling</li>
      <li style="text-align: justify">Parts of the development of the crawler <a
        href="https://github.com/dice-group/Squirrel/">Squirrel(Github)</a> are carried out by Geraldo Souza, Ivan
        Ermilov and Michael Röder.
      </li>
      <li style="text-align: justify">The Frontier has to be able to handle the sudden death of a worker. Blocked IP
        addresses have to be given to another worker if a timeout is reached. This includes a “keep alive” mechanism
        with which the workers can convince the frontier that they are still working.
      </li>
      <li style="text-align: justify">he Frontier has to be able to recrawl old URIs to update the data. There should be
        a default time duration after which the recrawling takes place. But it also has to be possible to set individual
        recrawling times for single URIs (e.g., if the worker found this information in an HTTP header field).
      </li>
      <li>Frontend for Frontier</li>
      <li>Shows statistics about status</li>
      <li>Number of crawled URIs</li>
      <li>Number of blocked IPs</li>
      <li>Length of queue</li>
      <li>......</li>
      <li>Statistics about workers</li>
      <li>Provenance</li>
      <li>Handled by sink</li>
      <li>Metadata</li>
      <li>Visualize the crawled graph</li>
      <li>E.g., gexf <a href="https://gephi.org/gexf/format/">GEXF File Format</a></li>
      <li>Triple store as sink</li>
      <li>1 graph per crawled URI</li>
      <li>1 metadata graph</li>
      <li>Deduplication</li>
      <li>Can be handled by reusing the triple store sink</li>
      <li>Check randomly chosen triples against store before processing data</li>
      <li>Team abilities</li>
      <li>Necessary</li>
      <li>Java programming</li>
      <li>Helpful</li>
      <li>Maven</li>
      <li>Docker</li>
      <li>Java web services</li>
      <li>1 developer with JavaScript experience</li>
      <li>Team management</li>
    </ul>

    <h3>
      <a id="user-content-steps" class="anchor" href="#steps" aria-hidden="true"></a>Steps
    </h3>
    <ul>
      <li style="text-align: justify">Get into the field of developing a crawler. An example for a linked data crawler
        is
        <a href="https://github.com/ldspider/ldspider">LDSpider(Github)</a> ,
        <a href="http://iswc2010.semanticweb.org/pdf/495.pdf">LDSpider: An open-source crawling framework for the Web of
          Linked Data</a>.
        However, it has its limitations.
      </li>
      <li style="text-align: justify">
        The deduplication is mainly build on the assumption that the sink is a triple store. All other parts have no
        specific order.
      </li>
    </ul>

  </div>
</div>

</body>
</html>
